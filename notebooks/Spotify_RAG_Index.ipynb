{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify RAG Index (Colab)\n",
    "Notebook ini membangun index FAISS dan file metadata dari dataset Spotify (Kaggle) untuk digunakan sebagai RAG di FastAPI.\n",
    "\n",
    "## Langkah-langkah:\n",
    "1. Install dependencies.\n",
    "2. Upload dataset Spotify (CSV).\n",
    "3. Normalisasi kolom dan pembersihan data.\n",
    "4. Buat embeddings dengan sentence-transformers.\n",
    "5. Bangun index FAISS dan simpan ke file.\n",
    "6. Export output (parquet, index, embeddings, metadata).\n",
    "7. Download hasil dan copy ke project `data/spotify_index/`.\n",
    "\n",
    "Catatan: Dataset yang didukung memiliki kolom seperti `track_name`, `artists`, `track_genre/genre`, `valence`, `energy`, `tempo`, `liveness`, `acousticness`, `instrumentalness`, `danceability`, `duration_ms`, `explicit`, `year`, `id/track_id`, `uri` (opsional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies\n",
    "!pip -q install pandas pyarrow numpy sentence-transformers faiss-cpu fastparquet\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "\n",
    "EMBED_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "OUTPUT_DIR = 'spotify_index_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('Deps OK, output dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset CSV\n",
    "Upload file CSV dari Kaggle (contoh: `spotify_tracks.csv`). Jika dataset Anda punya nama berbeda, sesuaikan variabel `CSV_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "CSV_PATH = list(uploaded.keys())[0]\n",
    "print('CSV_PATH:', CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dan normalisasi kolom\n",
    "Menyesuaikan berbagai kemungkinan nama kolom dari dataset Spotify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print('Rows:', len(df))\n",
    "# Normalisasi nama kolom\n",
    "cols = {c.lower(): c for c in df.columns}\n",
    "def pick(*names):\n",
    "    for n in names:\n",
    "        if n in cols: return cols[n]\n",
    "    return None\n",
    "\n",
    "col_title = pick('track_name','name','title')\n",
    "col_artist = pick('artists','artist_name','artist')\n",
    "col_genre = pick('track_genre','genre','genres')\n",
    "col_valence = pick('valence')\n",
    "col_energy = pick('energy')\n",
    "col_tempo = pick('tempo')\n",
    "col_liveness = pick('liveness')\n",
    "col_acousticness = pick('acousticness')\n",
    "col_instrumentalness = pick('instrumentalness')\n",
    "col_danceability = pick('danceability')\n",
    "col_speechiness = pick('speechiness')\n",
    "col_duration = pick('duration_ms','duration')\n",
    "col_explicit = pick('explicit')\n",
    "col_year = pick('year','release_year')\n",
    "col_id = pick('id','track_id','spotify_id')\n",
    "col_uri = pick('uri','track_uri')\n",
    "\n",
    "required = [col_title, col_artist, col_genre, col_valence, col_energy, col_tempo, col_liveness, col_acousticness, col_instrumentalness, col_danceability, col_duration]\n",
    "missing = [n for n in required if n is None]\n",
    "if missing:\n",
    "    raise ValueError(f'Kolom penting hilang: {missing}')\n",
    "\n",
    "work = pd.DataFrame({\n",
    "    'title': df[col_title].astype(str).str.strip(),\n",
    "    'artist': df[col_artist].astype(str).str.strip(),\n",
    "    'genres': df[col_genre].astype(str).str.lower().str.replace(';', ',').str.replace('|', ',').str.strip(),\n",
    "    'valence': pd.to_numeric(df[col_valence], errors='coerce'),\n",
    "    'energy': pd.to_numeric(df[col_energy], errors='coerce'),\n",
    "    'tempo': pd.to_numeric(df[col_tempo], errors='coerce'),\n",
    "    'liveness': pd.to_numeric(df[col_liveness], errors='coerce'),\n",
    "    'acousticness': pd.to_numeric(df[col_acousticness], errors='coerce'),\n",
    "    'instrumentalness': pd.to_numeric(df[col_instrumentalness], errors='coerce'),\n",
    "    'danceability': pd.to_numeric(df[col_danceability], errors='coerce'),\n",
    "    'speechiness': pd.to_numeric(df[col_speechiness], errors='coerce') if col_speechiness else np.nan,\n",
    "    'duration_ms': pd.to_numeric(df[col_duration], errors='coerce'),\n",
    "    'explicit': df[col_explicit] if col_explicit else False,\n",
    "    'year': pd.to_numeric(df[col_year], errors='coerce') if col_year else np.nan,\n",
    "    'track_id': df[col_id] if col_id else None,\n",
    "    'uri': df[col_uri] if col_uri else None,\n",
    "})\n",
    "\n",
    "# Drop baris dengan nilai penting yang hilang\n",
    "work = work.dropna(subset=['title','artist','duration_ms','valence','energy','tempo','liveness','acousticness','instrumentalness','danceability'])\n",
    "# Hapus duplikat berdasarkan title+artist\n",
    "work['key_unique'] = (work['title'].str.lower() + '|' + work['artist'].str.lower())\n",
    "work = work.drop_duplicates(subset=['key_unique'])\n",
    "\n",
    "# Buat teks pencarian untuk embeddings\n",
    "work['search_text'] = work['title'] + ' - ' + work['artist'] + ' [' + work['genres'].fillna('') + ']'\n",
    "work = work.reset_index(drop=True)\n",
    "print('Setelah bersih:', len(work))\n",
    "work.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings dengan Sentence-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(EMBED_MODEL)\n",
    "emb = model.encode(work['search_text'].tolist(), batch_size=256, show_progress_bar=True, normalize_embeddings=True)\n",
    "emb = np.asarray(emb, dtype='float32')\n",
    "np.save(os.path.join(OUTPUT_DIR, 'embeddings.npy'), emb)\n",
    "print('Embeddings shape:', emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bangun FAISS index (cosine via inner-product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(emb)\n",
    "faiss.write_index(index, os.path.join(OUTPUT_DIR, 'faiss.index'))\n",
    "print('FAISS index ditulis:', index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpan metadata (parquet + json info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['title','artist','genres','duration_ms','valence','energy','tempo','liveness','acousticness','instrumentalness','danceability','speechiness','explicit','year','track_id','uri']\n",
    "meta = work[meta_cols].copy()\n",
    "meta.to_parquet(os.path.join(OUTPUT_DIR, 'tracks.parquet'), index=False)\n",
    "with open(os.path.join(OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
    "    json.dump({\n",
    "        'created_at': datetime.utcnow().isoformat(),\n",
    "        'rows': int(len(meta)),\n",
    "        'embedding_model': EMBED_MODEL,\n",
    "        'fields': meta_cols\n",
    "    }, f, indent=2)\n",
    "print('Metadata tersimpan:', len(meta))\n",
    "meta.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip dan download output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "zip_path = 'spotify_rag_index.zip'\n",
    "shutil.make_archive('spotify_rag_index', 'zip', OUTPUT_DIR)\n",
    "files.download(zip_path)\n",
    "print('Siap diunduh:', zip_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}